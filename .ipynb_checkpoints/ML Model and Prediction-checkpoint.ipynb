{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split data into Training and Test sets\n",
    "\n",
    "The first step in order to build our model for the Iris dataset, given that initial EDA did not indicate the need for further data preprocessing, is to split our dataset. We'll fit our model on the training set, and test it on the test set.  For this purpose we'll use scikit-learn's built-in \"train_test_split\" function. This function shuffles and splits our initial dataset into a training set with 75% of the samples, and a tess set with the remaining 30% of our observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: \n",
      "(112, 4)\n",
      "\n",
      "y_train shape: \n",
      "(112,)\n",
      "\n",
      "X_test shape: \n",
      "(38, 4)\n",
      "\n",
      "y_test shape: \n",
      "(38,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \\n{}\".format(X_train.shape))\n",
    "print(\"\\n\" + \"y_train shape: \\n{}\".format(y_train.shape))\n",
    "print(\"\\n\" + \"X_test shape: \\n{}\".format(X_test.shape))\n",
    "print(\"\\n\" + \"y_test shape: \\n{}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Visualization\n",
    "\n",
    "Before blindly fitting models or any sophisticated machine learning technique, it's vital to inspect our data. This step may even show us that no modelling is necessary at all! Besides, abnormal measurements (outliers, missing data, or any other inconsistency) can be identified in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
